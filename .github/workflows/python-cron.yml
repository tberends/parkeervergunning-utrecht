name: Scrape website, process to csv and commit the file

on:
  schedule:
    # Runs every day at 17:00 UTC
    - cron: '00 17 * * *'
  workflow_dispatch:  # Voegt mogelijkheid toe om workflow handmatig te starten

jobs:
  generate-and-commit-csv:
    runs-on: ubuntu-latest
    permissions:                # Job-level permissions configuration starts hier
      contents: write           # 'write' access to repository contents
    steps:
      # Step 1: Checkout the repository
      - name: Checkout repository
        uses: actions/checkout@v2

      # Step 2: Setup Python environment
      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.13.1' # Install the Python version needed

      # Step 3: Install dependencies
      - name: Install dependencies
        run: pip install requests==2.32.3 bs4==0.0.2 pandas==2.2.3 urllib3==2.3.0

      # Step 4: Execute the Python script to create a CSV
      - name: Generate CSV
        run: python main.py

      # Step 5: Commit and push the generated CSV file if there are changes
      - name: Commit files
        id: commit
        run: |
          git config --local user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          git add data/*
          if [ -n "$(git status --porcelain)" ]; then
            git commit -m "Update csvfile"
            echo "changes=true" >> $GITHUB_ENV
          else
            echo "No changes to commit"
            echo "changes=false" >> $GITHUB_ENV
            exit 0
          fi        
      - name: Push changes
        if: success() && env.changes == 'true'
        uses: ad-m/github-push-action@master
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          branch: ${{ github.ref }}
          directory: .
          force: true
